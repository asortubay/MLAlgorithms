{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement parts of the Decision Tree Classification algorithm from scratch (i.e., without importing any libraries or packages for decision trees). A decision tree is a supervised learning model used for classification tasks and is comprised of the following steps:\n",
    "\n",
    "1. Split the Dataset: Create possible splits for each feature of the dataset. For each split, separate the data into two groups based on the feature's threshold value.\n",
    "\n",
    "2. Calculate the Split Metric: For each split, calculate an evaluation metric such as Gini Impurity to determine the best split. Gini Impurity measures how often a randomly chosen element would be incorrectly labeled, aiming to create pure groups. A Gini Impurity of 0 represents perfect purity, while higher values indicate more mixed groups.\n",
    "\n",
    "3. Build the Tree Recursively: Use a recursive function to create nodes, splitting the data until a stopping criterion is met (e.g., maximum depth of tree, minimum number of samples, or no more possible splits).\n",
    "\n",
    "4. Classify New Data: Use the resulting decision tree to classify a new data point based on the feature values and decision rules at each node.\n",
    "\n",
    "You will be given a 2D array of float values, train_data, as training data, where each subarray represents a unique sample. The last element in each subarray represents the true class label (e.g., 0 or 1). You will also be given test_data, which you need to classify using the decision tree you build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[2.8, 1.0, 0], \n",
    " [1.3, 3.1, 1], \n",
    " [3.6, 2.7, 0], \n",
    " [2.9, 1.9, 1], \n",
    " [1.5, 0.9, 0], \n",
    " [3.7, 1.5, 1]]\n",
    "\n",
    "\n",
    "test_data = [[3.0, 1.0], \n",
    " [1.8, 2.5], \n",
    " [2.2, 1.7]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton code\n",
    "\n",
    "# Step 1: Split dataset based on a feature and a threshold\n",
    "def split_dataset(data: list[list[float]], feature_index: int, threshold: float) -> tuple[list[list[float]], list[list[float]]]:\n",
    "    \"\"\"\n",
    "    Split the dataset into two groups based on a feature index and a threshold value.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to be split.\n",
    "        feature_index: Index of the feature to split on.\n",
    "        threshold: Threshold value for splitting.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two lists, each representing a group of data points.\n",
    "    \"\"\"\n",
    "    left, right = [], []\n",
    "    for row in data:\n",
    "        if row[feature_index] < threshold:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Step 2: Calculate Gini Impurity or Information Gain\n",
    "def calculate_gini(groups: list[list[list[float]]], classes: list[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Gini Impurity for a given dataset split.\n",
    "\n",
    "    Args:\n",
    "        groups: A list containing two groups of data points.\n",
    "        classes: A list of unique class labels.\n",
    "\n",
    "    Returns:\n",
    "        Gini Impurity of the split.\n",
    "    \"\"\"\n",
    "    n_samples = sum([len(group) for group in groups])\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        unique_labels = set(row[-1] for row in group)\n",
    "        for label in unique_labels:\n",
    "            count = sum(1 for row in group if row[-1] == label)\n",
    "            p = count / size\n",
    "            score += p ** 2\n",
    "        gini += (1.0 - score) * (size / n_samples)\n",
    "    return gini\n",
    "\n",
    "\n",
    "# Step 3: Select the best split point\n",
    "def get_best_split(data: list[list[float]]) -> dict[str, list[int, float, tuple[list[list[float]], list[list[float]]]]]:\n",
    "    \"\"\"\n",
    "    Find the best split point for the dataset.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to find the best split for.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the best feature index, threshold, and groups.\n",
    "    \"\"\"\n",
    "    class_values = list(set(row[-1] for row in data))\n",
    "    best_feature, best_threshold, best_score, best_groups = None, None, float('inf'), None\n",
    "\n",
    "    for feature_index in range(len(data[0]) - 1):\n",
    "        unique_values = set(row[feature_index] for row in data)\n",
    "        for threshold in unique_values:\n",
    "            groups = split_dataset(data, feature_index, threshold)\n",
    "            gini = calculate_gini(groups, class_values)\n",
    "            if gini < best_score:\n",
    "                best_feature, best_threshold, best_score, best_groups = feature_index, threshold, gini, groups\n",
    "\n",
    "    return {'feature_index': best_feature, 'threshold': best_threshold, 'groups': best_groups}\n",
    "\n",
    "# Helper function to create a terminal node\n",
    "def to_terminal(group: list[list[float]]) -> int:\n",
    "    \"\"\"\n",
    "    Create a terminal node.\n",
    "\n",
    "    Args:\n",
    "        group: A list of data points in a group.\n",
    "\n",
    "    Returns:\n",
    "        The class label that occurs most frequently in the group.\n",
    "    \"\"\"\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Step 4: Build the tree\n",
    "def build_tree(data: list[list[float]], max_depth: int, min_size: int, depth: int = 0) -> dict[str,list[int, float, dict]]:\n",
    "    \"\"\"\n",
    "    Build the decision tree recursively.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to build the tree from.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_size: The minimum number of samples required to split a node.\n",
    "        depth: The current depth of the tree (used during recursion).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the decision tree.\n",
    "    \"\"\"\n",
    "    split = get_best_split(data)\n",
    "    left_group, right_group = split['groups']\n",
    "\n",
    "    if not left_group or not right_group:\n",
    "        return to_terminal(left_group + right_group)\n",
    "\n",
    "    if depth >= max_depth:\n",
    "        return {'left': to_terminal(left_group), 'right': to_terminal(right_group)}\n",
    "\n",
    "    node = {'feature_index': split['feature_index'], 'threshold': split['threshold'], 'left': None, 'right': None}\n",
    "\n",
    "    if len(left_group) <= min_size:\n",
    "        node['left'] = to_terminal(left_group)\n",
    "    else:\n",
    "        node['left'] = build_tree(left_group, max_depth, min_size, depth + 1)\n",
    "\n",
    "    if len(right_group) <= min_size:\n",
    "        node['right'] = to_terminal(right_group)\n",
    "    else:\n",
    "        node['right'] = build_tree(right_group, max_depth, min_size, depth + 1)\n",
    "\n",
    "    return node\n",
    "\n",
    "# Step 5: Make a prediction\n",
    "def predict(node: dict[str, list[int, float, dict]], row: list[float]) -> int:\n",
    "    \"\"\"\n",
    "    Make a prediction for a given data point by traversing the decision tree.\n",
    "\n",
    "    Args:\n",
    "        node: The decision tree.\n",
    "        row: The data point to classify.\n",
    "\n",
    "    Returns:\n",
    "        The predicted class label.\n",
    "    \"\"\"\n",
    "    if row[node['feature_index']] < node['threshold']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Step 6: Decision Tree Classifier\n",
    "def decision_tree(train_data: list[list[float]], test_data: list[list[float]], max_depth: int, min_size: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Build the decision tree and classify the given test data.\n",
    "\n",
    "    Args:\n",
    "        train_data: The training dataset.\n",
    "        test_data: The dataset to classify.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_size: The minimum number of samples required to split a node.\n",
    "\n",
    "    Returns:\n",
    "        A list of predicted class labels for each sample in test_data.\n",
    "    \"\"\"\n",
    "    tree = build_tree(train_data, max_depth, min_size)\n",
    "    predictions = [predict(tree, row) for row in test_data]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = decision_tree(train_data, test_data, 100, 10 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medimagclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
