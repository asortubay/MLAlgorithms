{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GMM is an unsupervised learning algorithm that models a dataset using a mixture of several Gaussian distributions. It typically uses the Expectation-Maximization (EM) algorithm to estimate the parameters of these Gaussian components.\n",
    "\n",
    "The main steps for implementing GMM are:\n",
    "\n",
    "1. Initialize Parameters: Randomly initialize the parameters for each Gaussian component, including the means, variances, and weights.\n",
    "2. Expectation Step (E-Step): Calculate the responsibilities (probability of each data point belonging to each Gaussian).\n",
    "3. Maximization Step (M-Step): Update the parameters of the Gaussian components using the calculated responsibilities.\n",
    "4. Iterate Until Convergence: Repeat the E-Step and M-Step until the log-likelihood converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeleton code\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Step 1: Initialize parameters\n",
    "def initialize_parameters(data: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Initialize the means, variances, and weights of the Gaussian components.\n",
    "\n",
    "    Args:\n",
    "        data: Dataset of shape (n_samples, n_features).\n",
    "        n_components: Number of Gaussian components.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing initial means, variances, and weights.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    means = data[np.random.choice(n_samples, n_components, replace=False)]\n",
    "    variances = np.array([np.eye(n_features) for _ in range(n_components)])\n",
    "    weights = np.ones(n_components) / n_components\n",
    "    return means, variances, weights\n",
    "\n",
    "\n",
    "# Step 2: Calculate Gaussian probability density function (PDF)\n",
    "def gaussian_pdf(x: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Gaussian probability density function.\n",
    "\n",
    "    Args:\n",
    "        x: Data point of shape (n_features,).\n",
    "        mean: Mean of the Gaussian of shape (n_features,).\n",
    "        cov: Covariance matrix of shape (n_features, n_features).\n",
    "\n",
    "    Returns:\n",
    "        The probability density value of the data point.\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    det_cov = np.linalg.det(cov)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    norm_factor = 1 / ((2 * np.pi) ** (n / 2) * (det_cov ** 0.5))\n",
    "    exponent = -0.5 * np.dot(np.dot((x - mean).T, inv_cov), (x - mean))\n",
    "    return norm_factor * np.exp(exponent)\n",
    "\n",
    "\n",
    "# Step 3: Expectation Step (E-Step)\n",
    "def expectation_step(data: np.ndarray, means: np.ndarray, variances: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform the E-Step to calculate the responsibilities.\n",
    "\n",
    "    Args:\n",
    "        data: Dataset of shape (n_samples, n_features).\n",
    "        means: Means of the Gaussian components.\n",
    "        variances: Covariance matrices of the Gaussian components.\n",
    "        weights: Weights of the Gaussian components.\n",
    "\n",
    "    Returns:\n",
    "        Responsibilities matrix of shape (n_samples, n_components).\n",
    "    \"\"\"\n",
    "    n_samples, n_components = data.shape[0], means.shape[0]\n",
    "    responsibilities = np.zeros((n_samples, n_components))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_components):\n",
    "            responsibilities[i, j] = weights[j] * gaussian_pdf(data[i], means[j], variances[j])\n",
    "        responsibilities[i, :] /= np.sum(responsibilities[i, :])\n",
    "\n",
    "    return responsibilities\n",
    "\n",
    "\n",
    "# Step 4: Maximization Step (M-Step)\n",
    "def maximization_step(data: np.ndarray, responsibilities: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform the M-Step to update the parameters of the Gaussian components.\n",
    "\n",
    "    Args:\n",
    "        data: Dataset of shape (n_samples, n_features).\n",
    "        responsibilities: Responsibilities matrix of shape (n_samples, n_components).\n",
    "\n",
    "    Returns:\n",
    "        Updated means, variances, and weights of the Gaussian components.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    n_components = responsibilities.shape[1]\n",
    "    means = np.zeros((n_components, n_features))\n",
    "    variances = np.zeros((n_components, n_features, n_features))\n",
    "    weights = np.zeros(n_components)\n",
    "\n",
    "    for j in range(n_components):\n",
    "        responsibility_sum = np.sum(responsibilities[:, j])\n",
    "        means[j] = np.sum(responsibilities[:, j, None] * data, axis=0) / responsibility_sum\n",
    "        weights[j] = responsibility_sum / n_samples\n",
    "\n",
    "        # Update covariance matrices\n",
    "        cov_matrix = np.zeros((n_features, n_features))\n",
    "        for i in range(n_samples):\n",
    "            diff = (data[i] - means[j]).reshape(-1, 1)\n",
    "            cov_matrix += responsibilities[i, j] * np.dot(diff, diff.T)\n",
    "        variances[j] = cov_matrix / responsibility_sum\n",
    "\n",
    "    return means, variances, weights\n",
    "\n",
    "\n",
    "# Step 5: Log-Likelihood Calculation\n",
    "def log_likelihood(data: np.ndarray, means: np.ndarray, variances: np.ndarray, weights: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the log-likelihood of the data under the current GMM parameters.\n",
    "\n",
    "    Args:\n",
    "        data: Dataset of shape (n_samples, n_features).\n",
    "        means: Means of the Gaussian components.\n",
    "        variances: Covariance matrices of the Gaussian components.\n",
    "        weights: Weights of the Gaussian components.\n",
    "\n",
    "    Returns:\n",
    "        Log-likelihood value.\n",
    "    \"\"\"\n",
    "    n_samples, n_components = data.shape[0], means.shape[0]\n",
    "    log_likelihood_value = 0.0\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        prob_sum = 0.0\n",
    "        for j in range(n_components):\n",
    "            prob_sum += weights[j] * gaussian_pdf(data[i], means[j], variances[j])\n",
    "        log_likelihood_value += np.log(prob_sum)\n",
    "\n",
    "    return log_likelihood_value\n",
    "\n",
    "\n",
    "# Step 6: Iterate until convergence\n",
    "def fit_gmm(data: np.ndarray, n_components: int, max_iterations: int = 100, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Fit the GMM to the data using the EM algorithm.\n",
    "\n",
    "    Args:\n",
    "        data: Dataset of shape (n_samples, n_features).\n",
    "        n_components: Number of Gaussian components.\n",
    "        max_iterations: Maximum number of iterations.\n",
    "        tol: Convergence tolerance for log-likelihood.\n",
    "\n",
    "    Returns:\n",
    "        Final means, variances, and weights of the Gaussian components.\n",
    "    \"\"\"\n",
    "    means, variances, weights = initialize_parameters(data, n_components)\n",
    "    prev_log_likelihood = None\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        # E-Step\n",
    "        responsibilities = expectation_step(data, means, variances, weights)\n",
    "\n",
    "        # M-Step\n",
    "        means, variances, weights = maximization_step(data, responsibilities)\n",
    "\n",
    "        # Calculate log-likelihood\n",
    "        current_log_likelihood = log_likelihood(data, means, variances, weights)\n",
    "\n",
    "        # Check for convergence\n",
    "        if prev_log_likelihood is not None and abs(current_log_likelihood - prev_log_likelihood) < tol:\n",
    "            break\n",
    "\n",
    "        prev_log_likelihood = current_log_likelihood\n",
    "\n",
    "    return means, variances, weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medimagclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
